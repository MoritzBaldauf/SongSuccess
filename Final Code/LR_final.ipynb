{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading all the libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting up Spark and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Spark\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[8]\") \\\n",
    "    .appName(\"LogisticRegression\") \\\n",
    "    .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Loading the data\n",
    "try:\n",
    "    file = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"multiline\", \"true\").option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\").load(\"../Final Preprocessing/final.csv\")\n",
    "except Exception as e:\n",
    "    print(\"Error occured during loading the file:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature selection and transformation(with numerical variables as features for the regression model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the first step in our analysis, we use the numerical columns in our dataset as inputs in the logistic regression model. Now, we need to make sure the input variables in the logistic regression model is ready to work with. Logistic regression models needs input variables to be numerical. However, in our dataset, these columns that should contain numerical information are read as strings. So, we transform the type of data by using **withColumn** in PySpark as it allows us to modify the datatypes in our target columns nicely. We use it to cast each of the selected columns into \"double\" datatype, making sure that all the features are numerical and suitable for the logistic regression. As logistic regression also needs binary numerical variable as the output variable, we cast the \"billboard\" into a double."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Selecting the features and transforming data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Including numeric columns as features in the regression model(they contain numeric information but their datatype right now is strings)\n",
    "numeric_columns = [\"popularity\",\n",
    "                   \"duration_ms\", \n",
    "                   \"danceability\", \n",
    "                   \"energy\", \n",
    "                   \"key\", \n",
    "                   \"loudness\",\n",
    "                   \"mode\", \n",
    "                   \"speechiness\", \n",
    "                   \"acousticness\", \n",
    "                   \"instrumentalness\", \n",
    "                   \"liveness\", \n",
    "                   \"valence\",\n",
    "                   \"tempo\", \n",
    "                   \"time_signature\"]\n",
    "\n",
    "try:\n",
    "    # Converting the string columns to numeric\n",
    "    for column in numeric_columns:\n",
    "        file = file.withColumn(column, col(column).cast(\"double\"))\n",
    "        \n",
    "except AnalysisException as e:\n",
    "    print(\"Error when transforming the numeric columns:\", e)\n",
    "    \n",
    "    \n",
    "try:\n",
    "    # Converting the billboard column to numeric\n",
    "     file = file.withColumn(\"billboard\", col(\"billboard\").cast(\"double\"))\n",
    "\n",
    "except AnalysisException as e:\n",
    "    print(\"Error when transforming the billboard column:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Dropping potential NA's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure reliability and integrity in our model, we check for and drop any null values contained in the numeric_columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values in the numeric columns and drop them\n",
    "file = file.dropna(subset=numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Assembling the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, the model expects a single vector that contains all the features. That is why, in the next step, we use **VectorAssembler** to combine all the feature columns(numeric_columns) into a single vector. We name this single vector as \"unscaledFeatures\" as this will make it easier to distinguish in our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    # Assembling all the features into one vector\n",
    "    assembler = VectorAssembler(inputCols = numeric_columns, outputCol = \"unscaledFeatures\")\n",
    "    file = assembler.transform(file)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error when assembling the features:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scaling the features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we scale the features we have chosen to further prepare it for to be used in the model. Scaling is an important step as it will help make sure that all features contribute equally to the model by normalizing the data, which then will lead to better performance of the model overall. We scale the features here using **StandardScaler** in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Scaling the features\n",
    "    scaler = StandardScaler(inputCol = \"unscaledFeatures\", outputCol = \"scaledFeatures\")\n",
    "    file = scaler.fit(file).transform(file)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error when scaling the features:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Splitting into training and test dataset (Starting with the entire data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split our dataset into training and test data with 80/20 split. We defined the speficic seed for reproducibility and comparability with our other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and testing sets (using the entire data)\n",
    "train_data, test_data = file.randomSplit([0.8, 0.2], seed = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Alternative(Creating balanced dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our dataset includes many more songs that didn't end up on billboard than the ones that did, there will be some bias of any models built on that dataset to be classified as \"non-billboard\". Thus, to potentially avoid this issue and to take the bias into account, we also create a balanced dataset with 1:1 distribution of both classes. We name this dataframe, \"balanced_df\", and from this step onwards, we will calculate everything on both the original data and this balanced data to get the whole picture. The downside of creating this evenly split dataset may be that a lot of entries in our data with songs that are not on billboard gets eliminated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit to Moritz\n",
    "\n",
    "label_counts = file.groupBy(\"billboard\").count().collect()\n",
    "\n",
    "min_count = min(row[\"count\"] for row in label_counts)\n",
    "\n",
    "# Create balanced DataFrame by sampling\n",
    "balanced_df = None\n",
    "\n",
    "for row in label_counts:\n",
    "    label = row[\"billboard\"]\n",
    "    count = row[\"count\"]\n",
    "    fraction = min_count / count  # Calculate the fraction of the data to sample\n",
    "    \n",
    "    # Sample the data\n",
    "    sampled_df = file.filter(col(\"billboard\") == label).sample(False, fraction, seed=1)\n",
    "    \n",
    "    # Append sampled data to the balanced DataFrame\n",
    "    if balanced_df is None:\n",
    "        balanced_df = sampled_df.limit(min_count)  # Use limit to ensure exact number of instances\n",
    "    else:\n",
    "        balanced_df = balanced_df.union(sampled_df.limit(min_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|billboard|count|\n",
      "+---------+-----+\n",
      "|      0.0| 8825|\n",
      "|      1.0| 8923|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "balanced_df.select(\"billboard\").groupBy(\"billboard\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Splitting into training and test data (Starting with the balanced data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the balanced dataset, we split that data again into training and test data with 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and testing sets (using the balanced data)\n",
    "train_balanced_data, test_balanced_data = balanced_df.randomSplit([0.8, 0.2], seed = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our logistic regression model. We use the following parameters for it: \n",
    "- **scaledFeatures** as input features. As we know, they have been preprocessed and scaled.\n",
    "- **billboard** as output label. This column contains the target variable of whether a song is on billboard(1) or not(0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining logistic regression model\n",
    "lr = LogisticRegression(featuresCol = \"scaledFeatures\", labelCol = \"billboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Defining Parameters and Cross Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we definde a range of parameters for the logistic regression model. By doing so, we can achieve the optimal model performance. We use **ParamGridBuilder** to build a grid of parameter combinations that will be tested during the cross-validation. This helps us test the logistic regression model accross different settings and find the best set of parameters for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the parameter grid with a large range of parameters to explore different levels of regularization\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.25, 0.5, 0.75, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# https://spark.apache.org/docs/latest/ml-tuning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use **CrossValidator** for tuning parameters and selecting the models. This means we split the dataset into a number of folds and use the subset of the folds to train the model, and validating on the remaining fold. We've chosen 3-fold cross validation. This step is important to finding the logistic regression model with the best performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Using CrossValidator to find the best model\n",
    "    crossval = CrossValidator(estimator = lr,\n",
    "                              estimatorParamMaps = paramGrid,\n",
    "                              evaluator = BinaryClassificationEvaluator(labelCol = \"billboard\"),\n",
    "                              numFolds = 3)\n",
    "    # Running cross-validation and choosing the best set of parameters\n",
    "    cvModel = crossval.fit(train_data)  # using entire data\n",
    "    cvModel_balanced = crossval.fit(train_balanced_data)  # using the balanced data\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error when training the model:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, after running the cross-validations, we fetch the best modesl resulted from it. Next, we make predictions using the best models. We do this for both the test data from the entire dataset and the test data from the balanced dataset to have a wider outlook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the best models resulting from the cross-validation step\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel_balanced = cvModel_balanced.bestModel\n",
    "\n",
    "# Making predictions on the test data\n",
    "predictions = bestModel.transform(test_data)  # using entire data\n",
    "predictions_balanced = bestModel_balanced.transform(test_balanced_data)  # using balanced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use **BinaryClassificationEvaluator** in our logistic regression model to evaluate our models' performance. This evaluator is suitable to use in the logistic regression since our target variable is a binary variable with values 0 and 1. The configurations used are:\n",
    "\n",
    "- **labelCol** contains the true labels for the classification. In our case, it's the \"billboard_numeric\" column.\n",
    "- **rawPredictionCol** specifies the column containing raw prediction scores from the model\n",
    "- **metricName** specifies the metric used for evaluation of the model. The **areaUnderROC** evaluates model's ability to distinguish between positive and negative classes, with 1 indicating perfect classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the evaluator for binary classification - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.evaluation.BinaryClassificationEvaluator.html\n",
    "evaluator = BinaryClassificationEvaluator(labelCol = \"billboard\", rawPredictionCol = \"rawPrediction\", metricName = \"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Calculating areaUnderROC and areaUnderPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set AUC (ROC) for entire data = 0.6915915065465859\n",
      "Test set AUC (PR) for entire data = 0.26414429593305294\n",
      "Test set AUC (ROC) for balanced data = 0.6915915065465859\n",
      "Test set AUC (PR) for balanced data = 0.26414429593305294\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model - using the entire data\n",
    "areaUnderROC = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "areaUnderPR = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "print(\"Test set AUC (ROC) for entire data = \" + str(areaUnderROC))\n",
    "print(\"Test set AUC (PR) for entire data = \" + str(areaUnderPR))\n",
    "\n",
    "\n",
    "# Evaluate the model - using the entire data\n",
    "areaUnderROC_balanced = evaluator.evaluate(predictions_balanced, {evaluator.metricName: \"areaUnderROC\"})\n",
    "areaUnderPR_balanced = evaluator.evaluate(predictions_balanced, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "print(\"Test set AUC (ROC) for balanced data = \" + str(areaUnderROC))\n",
    "print(\"Test set AUC (PR) for balanced data = \" + str(areaUnderPR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the BinaryClassificationEvaluator doesn't support **Accuracy** as an evaluator metric, we need to manually calculate the accuracy of our model here. **Accuracy** is calculated by dividing the number of correct predictions by the total number of predictions. What we notice here is that the accuracy for the balanced dataset is much lower than the accuracy calculated by using the entire dataset. This is completely expected result, since our original dataset contains much more 0's in target variable than 1's, and when trained using the skewed data, it will predict the large portion to be 0, which will turn out accurate given the prevalence of 0s in the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Entire Dataset): 0.8604538087520259\n",
      "Accuracy (Balanced Dataset): 0.6246466930469191\n"
     ]
    }
   ],
   "source": [
    "# Calculating accuracy \n",
    "accuracy = predictions.filter(predictions.billboard == predictions.prediction).count() / float(predictions.count())\n",
    "print(f\"Accuracy (Entire Dataset): {accuracy}\")\n",
    "\n",
    "accuracy_balanced = predictions_balanced.filter(predictions_balanced.billboard == predictions_balanced.prediction).count() / float(predictions_balanced.count())\n",
    "print(f\"Accuracy (Balanced Dataset): {accuracy_balanced}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further inspection and visualization of the model performance, we create **Confusion Matrix** for both the entire dataset and balanced dataset results. Some notable insights include:\n",
    "- The balanced training seems to have largely improved the ability of the model to identify songs on the billboard, by reducing the False Negatives(FN) and increasing the True Positives(TP)\n",
    "- However, there is also a large increase in False Positives(FP) and this hints us the trade-off between making the model less conservative and making more positive predictions but at the cost of increased mistake in that area of prediction. \n",
    "- Overall, the model seem to have much more balanced predictions after eliminating the skewness of the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|billboard|prediction|count|\n",
      "+---------+----------+-----+\n",
      "|      1.0|       1.0|    8|\n",
      "|      0.0|       1.0|   12|\n",
      "|      1.0|       0.0| 1710|\n",
      "|      0.0|       0.0|10610|\n",
      "+---------+----------+-----+\n",
      "\n",
      "+---------+----------+-----+\n",
      "|billboard|prediction|count|\n",
      "+---------+----------+-----+\n",
      "|      1.0|       1.0| 1158|\n",
      "|      0.0|       1.0|  688|\n",
      "|      1.0|       0.0|  640|\n",
      "|      0.0|       0.0| 1052|\n",
      "+---------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "confusion_matrix = predictions.groupBy(\"billboard\", \"prediction\").count()\n",
    "confusion_matrix.show()\n",
    "\n",
    "confusion_matrix_balanced = predictions_balanced.groupBy(\"billboard\", \"prediction\").count()\n",
    "confusion_matrix_balanced.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Precision, Recall, and F1 Scores ( Additional Evaluation Metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of ROC-AUC, PR-AUC, and accuracy, making use of the confusion matrix we created, we can also calculate additional metrics for evaluation such as precision, recall, and F1 scores based on our balanced dataset, so that we have more broader picture when benchmarking the results with our other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision(Balanced Dataset): 0.6273022751895991\n",
      "Recall(Balanced Dataset): 0.6440489432703004\n",
      "F1 Score(Balanced Dataset): 0.6355653128430296\n"
     ]
    }
   ],
   "source": [
    "# Assigning the needed values from the confusion matrix\n",
    "\n",
    "TP = confusion_matrix_balanced.filter((col(\"billboard\") == 1) & (col(\"prediction\") == 1)).collect()[0][\"count\"]\n",
    "FP = confusion_matrix_balanced.filter((col(\"billboard\") == 0) & (col(\"prediction\") == 1)).collect()[0][\"count\"]\n",
    "FN = confusion_matrix_balanced.filter((col(\"billboard\") == 1) & (col(\"prediction\") == 0)).collect()[0][\"count\"]\n",
    "\n",
    "# Calculating Precision and Recall\n",
    "\n",
    "Precision_balanced = TP/(TP + FP) \n",
    "Recall_balanced = TP/(TP + FN) \n",
    "F1_balanced = 2 * (Precision_balanced * Recall_balanced) / (Precision_balanced + Recall_balanced)\n",
    "\n",
    "\n",
    "print(f\"Precision(Balanced Dataset): {Precision_balanced}\")\n",
    "print(f\"Recall(Balanced Dataset): {Recall_balanced}\")\n",
    "print(f\"F1 Score(Balanced Dataset): {F1_balanced}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Getting the coefficients of the features based on the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can also offer us extra insights into the model and its predictions is finding out the coefficients of each features within the model. By sorting in order of magnitude of the coefficients, we can get a insight on which variables have more influence on whether a song ends up on billboard. In other words, which characteristics of a song are more important to decide what ends up on billboard. On top of providing us with the magnitude of a feature's influence on the output variable, it can also show us whether that influence is negative or positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting coefficients and intercepts\n",
    "coefficients = bestModel_balanced.coefficients # used only the balanced dataset here as both of them(entire and balanced) showed very comparable results\n",
    "intercept = bestModel_balanced.intercept  # intercept tells us log-odds of target variable when all predictors are 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features, sorted by their importance:\n",
      "valence: 0.42657706555151625\n",
      "instrumentalness: -0.4224917545190806\n",
      "loudness: -0.3808035945347383\n",
      "acousticness: -0.37450232777374254\n",
      "energy: -0.17638056347508896\n",
      "mode: 0.1598584359155145\n",
      "liveness: -0.15633697691839823\n",
      "popularity: 0.08315368262359175\n",
      "tempo: -0.07379338312891608\n",
      "danceability: -0.049528215892181704\n",
      "speechiness: -0.04882585617490412\n",
      "duration_ms: -0.03614428911309265\n",
      "time_signature: -0.007931457372988714\n",
      "key: 0.0027720485185212407\n"
     ]
    }
   ],
   "source": [
    "# Mapping coefficients to feature names\n",
    "featureNames = numeric_columns\n",
    "featureCoefficients = [(feature, coeff) for feature, coeff in zip(featureNames, coefficients)]\n",
    "sorted_features = sorted(featureCoefficients, key = lambda x: abs(x[1]), reverse = True)  # sorting from the most to least important\n",
    "\n",
    "# Printing sorted features by importance\n",
    "print(\"Features, sorted by their importance:\")\n",
    "for feature, coefficient in sorted_features:\n",
    "    print(f\"{feature}: {coefficient}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the sorted features based on their coefficients, we see that **valence, instumentalness, acousticness, loudness, energy, mode, liveliness** seems to have more influence on the probability of a song ending up on billboard than the other feature variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "try:\n",
    "    bestModel_balanced.save(\"../Final Code/trained_models/LR_model\")\n",
    "    \n",
    "except Py4JJavaError:\n",
    "    print(\"Error saving model the model, make sure the model isn't allready saved in the defined folder\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
