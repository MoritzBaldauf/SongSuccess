{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Preprocessing\n",
    "\n",
    "This notebook contains the preprocessing conducted for the data to be used in the development of an NLP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Spark\n",
    "\n",
    "Start Spark, import necessary functions and register the UDFs from \"preproc.py\" and afterwards read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark/\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import collections\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"NLP1\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .config(\"spark.sql.random.seed\", \"1234\") \\\n",
    "   .getOrCreate()\n",
    "      \n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk) (4.51.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.8/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: langid in /opt/conda/lib/python3.8/site-packages (1.1.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from langid) (1.19.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Other packages\n",
    "import sys\n",
    "!{sys.executable} -m pip install nltk --no-cache-dir\n",
    "import sys\n",
    "!{sys.executable} -m pip install langid --no-cache-dir\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Register functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#register functions\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import preproc as pp\n",
    "\n",
    "\n",
    "# Refer to https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html\n",
    "# `pp.check_lang` is used to classify the language of our input text\n",
    "check_lang_udf = udf(pp.check_lang, StringType())\n",
    "\n",
    "\n",
    "# removes stop words (cleaned_str/row/document)\n",
    "remove_stops_udf = udf(pp.remove_stops, StringType())\n",
    "\n",
    "\n",
    "# catch-all to remove other 'words' that I felt didn't add a lot of value\n",
    "remove_features_udf = udf(pp.remove_features, StringType())\n",
    "\n",
    "\n",
    "# Process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech\n",
    "# tagging, POS-tagging, or simply tagging.\n",
    "# http://www.nltk.org/book/ch05.html\n",
    "tag_and_remove_udf = udf(pp.tag_and_remove, StringType())\n",
    "\n",
    "\n",
    "# lemmatize\n",
    "# http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "lemmatize_udf = udf(pp.lemmatize, StringType())\n",
    "\n",
    "\n",
    "# check to see if a row only contains whitespace\n",
    "check_blanks_udf = udf(pp.check_blanks, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+------+--------------+-----------+--------------------+---------+\n",
      "|_c0|          track_name|              artist|            track_id|          album_name|popularity|duration_ms|explicit|danceability|energy|key|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence| tempo|time_signature|track_genre|              lyrics|billboard|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+------+--------------+-----------+--------------------+---------+\n",
      "|  0|\"\"\"Martha: \"\"\"\"M'...|Friedrich von Flo...|1NzZWhNIP9DIX4yy0...|The World's Best ...|        23|     204706|   False|       0.222| 0.195|  5| -13.631|   1|     0.0368|       0.982|        0.000169|  0.0945|   0.11|89.332|             4|      opera|M'appar√¨ tutto am...|      0.0|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+------+--------------+-----------+--------------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.utils import AnalysisException \n",
    "\n",
    "try: \n",
    "    file = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"multiline\", \"true\").option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\").load(\"../Final Preprocessing/final.csv\")\n",
    "    file.show(1)\n",
    "except AnalysisException: \n",
    "    print(\"Please check the Filename and Filepath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61762"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check size of loaded data\n",
    "file.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Change the billboard variable into integers instead of strings\n",
    "\n",
    "Next we change the billboard variable from a string to integers, where 1 indicates that the song was in the billboard charts and 0 indicates that it was not. We do this because if the variable were a string, we would get an error during the model training phase. Be careful, do not run this cell twice without reading the file in again, otherwise the file might become empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from pyspark.sql.functions import col\\n\\n#file = file.filter(col(\"billboard\").isin(\"True\", \"False\", \"true\", \"false\"))\\nfile = file.withColumn(\\'billboard\\', when((col(\\'billboard\\') == True), 1).otherwise(0))'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from pyspark.sql.functions import col\n",
    "\n",
    "#file = file.filter(col(\"billboard\").isin(\"True\", \"False\", \"true\", \"false\"))\n",
    "file = file.withColumn('billboard', when((col('billboard') == True), 1).otherwise(0))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#file.printSchema()\\nfile.show(1)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#file.printSchema()\n",
    "file.show(1)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Select relevant columns and rename them\n",
    "\n",
    "Now we select only the variables of interest: lyrics of the song, the variable indicating if the song was in the billboard or not, and finally some id variable. We also rename the variables, because I previously made a mistake and instead of changing all my code, I chose to rename them with one extra line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = file.select(\"_c0\",\"billboard\",\"lyrics\")\n",
    "data = data.withColumnRenamed(\"_c0\", \"id\")\n",
    "data = data.withColumnRenamed(\"billboard\", \"label\")\n",
    "data = data.withColumnRenamed(\"lyrics\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Assign the language of each song to a new column\n",
    "\n",
    "Now we start preprocessing: Use a UDF to check the language of each song and add a new column to the data, indicating the language of the song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_df = data.withColumn(\"lang\", check_lang_udf(data[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create two datasets, one with all languages and one with only English\n",
    "\n",
    "In addition to the existing dataset, we create another one existing only of English songs. The idea behind this was that the model might perform better or worse on any of these two, because most billboard songs are English. To check whether this assumption was true, we created datasets of \"unfiltered\" songs (in any language) and of \"onlyEnglish\" songs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1: do not filter text by language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_df_unfiltered = lang_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 2: filter text by language\n",
    "\n",
    "We do this so that we can later see whether the model performs equally well when only being fed with English songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_df_onlyEnglish = lang_df.filter(lang_df[\"lang\"] == \"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the results might take a while:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#language_counts = lang_df.select('lang').groupBy('lang').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#language_counts = lang_df_onlyEnglish.select('lang').groupBy('lang').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Next we remove stop words\n",
    "\n",
    "From here on, we do every processing step twice: once for the data with all songs (unfiltered) and once for the data with only English songs (onlyEnglish). Every step follows the one before, meaning that the transformation of each step is invoked on the column that was created the step before. In this first step, we create a new column in which we copy the lyrics of the song, but remove the stop words defined by our UDF. Stop words are some of the most common words in a language. Removing them reduces dimensionality, so that our whole program is a bit faster. Also, since they are commonly used in most songs, it probably doesn't matter that much if they are in there or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_stops_unfiltered = lang_df_unfiltered.withColumn(\"stop_text\", remove_stops_udf(lang_df_unfiltered[\"text\"]))\n",
    "rm_stops_onlyEnglish = lang_df_onlyEnglish.withColumn(\"stop_text\", remove_stops_udf(lang_df_onlyEnglish[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Remove some other words that might not be necessary\n",
    "\n",
    "Next we use some sort of catch-all UDF that removes some other probably unnessary words or strings. This further reduces dimensionality, making it all go smoother and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_features_df_unfiltered = rm_stops_unfiltered.withColumn(\"feat_text\", remove_features_udf(rm_stops_unfiltered[\"stop_text\"]))\n",
    "rm_features_df_onlyEnglish = rm_stops_onlyEnglish.withColumn(\"feat_text\", remove_features_udf(rm_stops_onlyEnglish[\"stop_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. POS-tagging the words\n",
    "\n",
    "POS-tagging is the process of classifying the words into their word classes or lexical categories. That is, we classify words as different types of nouns, adjectives and verbs. The tagger can't only find which word class a word belongs to in a specific context, but it can also guess which word type a word is based on its root. After we tagged all the words, we only keep those which fit into our categories (aka all nouns, adjectives and verbs). I couldn't really figure out why we do that exactly, but my guess is to remove again decrease dimensionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_df_unfiltered = rm_features_df_unfiltered.withColumn(\"tagged_text\", tag_and_remove_udf(rm_features_df_unfiltered[\"feat_text\"]))\n",
    "tagged_df_onlyEnglish = rm_features_df_onlyEnglish.withColumn(\"tagged_text\", tag_and_remove_udf(rm_features_df_onlyEnglish[\"feat_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Lemmatize\n",
    "\n",
    "We lemmatize the words, meaning that we group certain words together and display only one word representing the whole group. As an example, we might group \"democracy\", \"democratic\" and \"democratization\" together and only display \"democracy\". This helps the model as there aren't as many different words anymore, because e.g. \"sing\" and \"singing\" are now the same. There aren't that many words to compare to each other anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_df_unfiltered = tagged_df_unfiltered.withColumn(\"lemm_text\", lemmatize_udf(tagged_df_unfiltered[\"tagged_text\"]))\n",
    "lemm_df_onlyEnglish = tagged_df_onlyEnglish.withColumn(\"lemm_text\", lemmatize_udf(tagged_df_onlyEnglish[\"tagged_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Check blanks\n",
    "\n",
    "We check whether there are cells of text with only whitespace. First, we check all rows and create a new column indicating \n",
    "whether a cell of the lemmatized text has only whitespace in it or not. We then remove such rows using .filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_blanks_df_unfiltered = lemm_df_unfiltered.withColumn(\"is_blank\", check_blanks_udf(lemm_df_unfiltered[\"lemm_text\"]))\n",
    "no_blanks_df_unfiltered = check_blanks_df_unfiltered.filter(check_blanks_df_unfiltered[\"is_blank\"] == \"False\")\n",
    "\n",
    "check_blanks_df_onlyEnglish = lemm_df_onlyEnglish.withColumn(\"is_blank\", check_blanks_udf(lemm_df_onlyEnglish[\"lemm_text\"]))\n",
    "no_blanks_df_onlyEnglish = check_blanks_df_onlyEnglish.filter(check_blanks_df_onlyEnglish[\"is_blank\"] == \"False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Deduplication\n",
    "\n",
    "We drop duplicates based on \"text\" (which represents the original column of lyrics) and \"label\" (which represents whether a song was in the charts or not). We shouldn't have to do this technically, but we still do it to be on the safe side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_df_unfiltered = no_blanks_df_unfiltered.dropDuplicates(['text', 'label'])\n",
    "dedup_df_onlyEnglish = no_blanks_df_onlyEnglish.dropDuplicates(['text', 'label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Select relevant data again\n",
    "\n",
    "Now we select the relevant columns, which are \"label\", \"id\" and \"lemm_text\" (which is the last version of our lyrics after all the transformations we did)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_unfiltered = dedup_df_unfiltered.select(dedup_df_unfiltered['id'], dedup_df_unfiltered['lemm_text'], dedup_df_unfiltered['label'])\n",
    "data_set_onlyEnglish = dedup_df_onlyEnglish.select(dedup_df_onlyEnglish['id'], dedup_df_onlyEnglish['lemm_text'], dedup_df_onlyEnglish['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_set_unfiltered.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_set_onlyEnglish.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Drop NAs\n",
    "\n",
    "We drop some NAs to be on the safe side (this doesn't work perfectly, in the end file there will still be some text columns with only whitespace, but we remove them in the other notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_unfiltered = data_set_unfiltered.na.drop()\n",
    "data_set_onlyEnglish = data_set_onlyEnglish.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Rename column\n",
    "\n",
    "We rename the \"lemm_text\" column to \"text\", as this is the name we use in the other notebooks as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unfiltered = data_set_unfiltered.withColumnRenamed(\"lemm_text\", \"text\")\n",
    "data_onlyEnglish = data_set_onlyEnglish.withColumnRenamed(\"lemm_text\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_unfiltered.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_onlyEnglish.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Save processed full data\n",
    "\n",
    "Finally we save the data. I commented that cell out, as it's already done, takes a \n",
    "while and since we already have the data there's no need to run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pandas\n",
    "data_unfiltered.toPandas().to_csv('../Final Preprocessing/NLP_processed_unfiltered.csv')\n",
    "data_onlyEnglish.toPandas().to_csv('../Final Preprocessing/NLP_processed_onlyEnglish.csv')\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
