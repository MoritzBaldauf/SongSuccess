{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing findspark and Reading In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark/\")\n",
    "from pyspark.sql import SparkSession\n",
    "import collections\n",
    "import pyspark.sql.functions as F # useful for preprocessing columns\n",
    "import shutil # used in removing folders when writing out our CSV files\n",
    "import os # used in renaming files when writing out our CSV files\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local[8]\") \\\n",
    "   .appName(\"RatingsHistogram\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    billboard = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"escapeQuotes\", \"true\").load(\"./data/charts.csv\")\n",
    "    spotify = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"escapeQuotes\", \"true\").load(\"./data/spotify.csv\")\n",
    "except Exception as e:\n",
    "    print(\"Input files not found. Please check that the Files are in the correct Folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading in the data, we also create a function called `output`, which is based on the `write.csv` function of Spark. It is used to automatically merge the partitions and place the created CSV output into the folder where the Jupyter Notebook is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(outdata, outname):\n",
    "    df = outdata.coalesce(1)\n",
    "    temp_dir = \"./out\"\n",
    "    output_file = outname + \".csv\"\n",
    "    df.write.csv(temp_dir, header=True)\n",
    "    temp_file_path = os.path.join(temp_dir, os.listdir(temp_dir)[0])\n",
    "    os.rename(temp_file_path, output_file)\n",
    "    shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Billboard and Spotify Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reach our goal of predicting whether a song will make it onto the Billboard Hot100 or not, we first need to join our two data sets together. For this,  **we need to change the Billboard data set's formatting** to match with the Spotify data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|   track_name|             artists|\n",
      "+-------------+--------------------+\n",
      "|   Easy On Me|               Adele|\n",
      "|         Stay|The Kid LAROI;Jus...|\n",
      "|Industry Baby|Lil Nas X;Jack Ha...|\n",
      "|   Fancy Like|        Walker Hayes|\n",
      "|   Bad Habits|          Ed Sheeran|\n",
      "+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Formating to make artist names be seperated by ; \n",
    "billboard2 = billboard.select(\"song\", \"artist\")\n",
    "billboard2 = billboard2.withColumn('artist', F.regexp_replace('artist', ' Featuring ', ';'))\n",
    "billboard2 = billboard2.withColumn('artist', F.regexp_replace('artist', ' & ', ';'))\n",
    "billboard2 = billboard2.withColumn('artist', F.regexp_replace('artist', ' x ', ';'))\n",
    "billboard2 = billboard2.withColumn('artist', F.regexp_replace('artist', ' X ', ';'))\n",
    "billboard2 = billboard2.selectExpr(\"song as track_name\", \"artist as artists\")\n",
    "billboard2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we <ins>create a subset</ins> for the songs that are on the Billboard Hot100 (`result1`) and those that aren't (`result2`). We add the fitting value in the new column (`billboard`) for each subset and then join them back together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+---+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+---------+\n",
      "|   track_name|             artists|_c0|            track_id|          album_name|popularity|duration_ms|explicit|danceability|energy|key|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|  tempo|time_signature|track_genre|billboard|\n",
      "+-------------+--------------------+---+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+---------+\n",
      "|Say Something|A Great Big World...|  6|6Vc5wAMmXdKIAM7WU...|Is There Anybody ...|        74|     229400|   False|       0.407| 0.147|  2|  -8.822|   1|     0.0355|       0.857|        2.89e-06|  0.0913| 0.0765|141.284|             3|   acoustic|     true|\n",
      "|    I'm Yours|          Jason Mraz|  7|1EzrEOXmMH3G43AXT...|We Sing. We Dance...|        80|     242946|   False|       0.703| 0.444| 11|  -9.331|   1|     0.0417|       0.559|             0.0|  0.0973|  0.712| 150.96|             4|   acoustic|     true|\n",
      "+-------------+--------------------+---+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result1 = spotify.join(billboard2, ['track_name','artists'], 'semi').withColumn('billboard', F.lit(True))\n",
    "result2 = spotify.join(billboard2, ['track_name','artists'], 'anti').withColumn('billboard', F.lit(False))\n",
    "result = result1.unionAll(result2)\n",
    "result.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we'll create our output to `result.csv`. For this, we use our previously created function `output`. However, this is only an intermediate file, and we only use it to split it and call the Musixmatch API on it. (We commented out the output as to not overwrite the already existing file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup = result.dropDuplicates(['track_name', 'artists'])\n",
    "dedup = dedup.where(dedup._c0 != 80832) # dropping one specific entry with faulty formatting\n",
    "\n",
    "# output(dedup, \"result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data for MusixMatch API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our joined data set, we need to **use the MusixMatch API to get the lyrics to our songs**. Unfortunately, this API comes with limitations of being able to only call for 2000 songs per day per key. To speed up the process, we split the data into 4 parts for each team member, so that we can join it back later when we have the lyrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create four csv files with 1/4 of the songs (around 20.000) \n",
    "# so that we can split up the API calling process  \n",
    "\n",
    "from pyspark.sql.functions import lit,row_number,col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window().partitionBy(lit('a')).orderBy(lit('a'))\n",
    "df = dedup.withColumn(\"row_num\", row_number().over(w))\n",
    "splitting = [1, 2, 3, 4]\n",
    "num = round(dedup.count() / 4)\n",
    "\n",
    "for i in splitting:\n",
    "    begin = (i - 1) * num \n",
    "    end = i * num - 1\n",
    "    dfnew = df.filter(col('row_num').between(begin,end))\n",
    "    dfnew.drop('row_num')\n",
    "    dfnew = dfnew.withColumnRenamed('_c0', 'id')\n",
    "    dfnew.toPandas().to_csv(f'./splits/split{i}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the MusixMatch API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is the same one that each one of our team members used to **get the lyrics** to their split of the data set. This is *just for demonstration purposes* given that each of us had to modify this each day for the appropriate song IDs and API keys. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important to note that we have **removed our API keys and client secrets** for security purposes. Please reach out to our team in case you would want to try out this part of code, or alternatively, input your Spotify and Musixmatch API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access Token: BQDKI7Utobbx6k47jJwhrmHTe52TdjS8_P6wYcxalCZ6VhTU-3t9duzFG2d8wXlubmxbpjcL2Xta6iwKYOSk786FjlW0MjJRcnZFOdj2Msf6dzmf7E0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import base64\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Spotify API Client ID and Secret\n",
    "# Note: we removed the keys for security purposes, if you want to try out the API calls, please reach out to our group\n",
    "client_id = 'SPOTIFY CLIENT ID HERE'\n",
    "client_secret = 'SPOTIFY SECRET HERE'\n",
    "\n",
    "# Encode client_id and client_secret\n",
    "auth_str = f\"{client_id}:{client_secret}\"\n",
    "auth_bytes = auth_str.encode('utf-8')\n",
    "auth_base64 = base64.b64encode(auth_bytes).decode('utf-8')\n",
    "\n",
    "# Set up the request headers and body\n",
    "headers = {\n",
    "    'Authorization': f'Basic {auth_base64}'\n",
    "}\n",
    "\n",
    "data = {\n",
    "    'grant_type': 'client_credentials'\n",
    "}\n",
    "\n",
    "# Make the POST request to the Spotify API token endpoint\n",
    "response = requests.post('https://accounts.spotify.com/api/token', headers=headers, data=data)\n",
    "\n",
    "# Now we can make the request, if response code is \"200\" it means success and then we can get the access token\n",
    "# The access token is valid for ONE HOUR, after that we need a new token\n",
    "if response.status_code == 200:\n",
    "    access_token = response.json().get('access_token')\n",
    "    print(f\"Access Token: {access_token}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve access token: {response.status_code}\")\n",
    "    print(response.json())\n",
    "    \n",
    "# Musixmatch API\n",
    "api_key = 'MUSIXMATCH API KEY HERE'\n",
    "\n",
    "# Define the endpoint and parameters for the API request\n",
    "base_url = 'https://api.musixmatch.com/ws/1.1/'\n",
    "track_search_endpoint = 'track.search'\n",
    "lyrics_get_endpoint = 'track.lyrics.get'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to search for a track on the Musixmatch API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up the APIs, we **created a function** which **returns the song's Spotify ID** given the track name and the artist. This will be the input to our next function which delivers the lyrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_track(track_name, artist_name):\n",
    "    search_url = base_url + track_search_endpoint\n",
    "    params = {\n",
    "        'q_track': track_name,\n",
    "        'q_artist': artist_name,\n",
    "        'apikey': api_key,\n",
    "        's_track_rating': 'desc',\n",
    "        'f_has_lyrics': 'true'\n",
    "    }\n",
    "    response = requests.get(search_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            result = response.json()\n",
    "            message = result.get('message')\n",
    "            if message:\n",
    "                body = message.get('body')\n",
    "                if body:\n",
    "                    track_list = body.get('track_list')\n",
    "                    if track_list:\n",
    "                        return track_list[0].get('track').get('track_id')\n",
    "            return \"No track found.\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing response: {e}\")\n",
    "            print(response.text)\n",
    "            return \"No track found.\"\n",
    "    else:\n",
    "        print(f\"Failed to search track: {response.status_code}\")\n",
    "        print(response.json())\n",
    "        return \"No track found.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get lyrics for a track on the Musixmatch API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we created our function `get_lyrics` which fetches the lyrics of the song based on the track ID from the Musixmatch API. It returns \"No lyrics found\" in case of a song not found. (However, it is important to note that the free version of the Musixmatch API only returns 30% of the lyrics for each song.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyrics(track_id):\n",
    "    lyrics_url = base_url + lyrics_get_endpoint\n",
    "    params = {\n",
    "        'track_id': track_id,\n",
    "        'apikey': api_key\n",
    "    }\n",
    "    response = requests.get(lyrics_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        try:\n",
    "            result = response.json()\n",
    "            message = result.get('message')\n",
    "            if message:\n",
    "                body = message.get('body')\n",
    "                if body:\n",
    "                    lyrics = body.get('lyrics')\n",
    "                    if lyrics:\n",
    "                        lyrics_body = lyrics.get('lyrics_body')\n",
    "                        lyrics_copyright = lyrics.get('lyrics_copyright', 'No lyrics found')\n",
    "                        return lyrics_body, lyrics_copyright\n",
    "            return \"No lyrics found.\", \"No lyrics found\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing response: {e}\")\n",
    "            print(response.text)\n",
    "            return \"No lyrics found.\", \"No lyrics found\"\n",
    "    else:\n",
    "        print(f\"Failed to get lyrics: {response.status_code}\")\n",
    "        print(response.json())\n",
    "        return \"No lyrics found.\", \"No lyrics found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To demonstrate**, we'll load the first 10 rows of the first split and show that it indeed gets the lyrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------+-----+--------------------+------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+---------+-------+\n",
      "|_c0|          track_name|artists|   id|            track_id|        album_name|popularity|duration_ms|explicit|danceability|energy|key|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|  tempo|time_signature|track_genre|billboard|row_num|\n",
      "+---+--------------------+-------+-----+--------------------+------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+---------+-------+\n",
      "|  0|\"\"\"Call Me - Sing...|Blondie|82022|3x0lg7bRDP2LCn65J...|80s Bangers Vol. 1|         6|     212706|   False|       0.557| 0.832|  2|  -6.708|   0|     0.0325|    0.000868|         0.00128|  0.0975|   0.75|142.702|             4|  power-pop|    False|      1|\n",
      "+---+--------------------+-------+-----+--------------------+------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+-------+--------------+-----------+---------+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"escapeQuotes\", \"false\").load(\"./splits/split1.csv\")\n",
    "from pyspark.sql.functions import lit,row_number,col\n",
    "subset = file.filter(col('_c0').between(0, 10))\n",
    "subset.show(n = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we defined **a new function so that we can use RDD mapping** on the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customFunction(row):\n",
    "    track_id = search_track(row.track_name, row.artists)\n",
    "    lyrics, lyrics_copyright = get_lyrics(track_id)\n",
    "    return row + (lyrics,)\n",
    "subset2 = subset.rdd.map(customFunction)\n",
    "\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql import Row\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.createDataFrame(subset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------------------------------------------------------+-------+-----+----------------------+------------------+---+------+-----+-----+-----+---+------+---+------+--------+-------+------+----+-------+---+---------+-----+---+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|_1 |_2                                                                 |_3     |_4   |_5                    |_6                |_7 |_8    |_9   |_10  |_11  |_12|_13   |_14|_15   |_16     |_17    |_18   |_19 |_20    |_21|_22      |_23  |_24|_25                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+---+-------------------------------------------------------------------+-------+-----+----------------------+------------------+---+------+-----+-----+-----+---+------+---+------+--------+-------+------+----+-------+---+---------+-----+---+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0  |\"\"\"Call Me - Single Version / Theme From \"\"\"\"American Gigolo\"\"\"\"\"\"\"|Blondie|82022|3x0lg7bRDP2LCn65JeNdwL|80s Bangers Vol. 1|6  |212706|False|0.557|0.832|2  |-6.708|0  |0.0325|0.000868|0.00128|0.0975|0.75|142.702|4  |power-pop|False|1  |Colour me your colour, baby\n",
      "Colour me your car\n",
      "Colour me your colour, darling\n",
      "I know who you are\n",
      "\n",
      "Come up off your colour chart\n",
      "I know where you're coming from\n",
      "\n",
      "Call me (Call me) on the line\n",
      "Call me, call me any, anytime\n",
      "Call me (Call me) I'll arrive\n",
      "You can call me any day or night\n",
      "Call me\n",
      "\n",
      "Cover me with kisses, baby\n",
      "...\n",
      "\n",
      "******* This Lyrics is NOT for Commercial use *******\n",
      "(1409624629210)|\n",
      "+---+-------------------------------------------------------------------+-------+-----+----------------------+------------------+---+------+-----+-----+-----+---+------+---+------+--------+-------+------+----+-------+---+---------+-----+---+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n = 1, truncate = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining our lyrics, we **collected all of the split up data sets** into our `merge` folder. (<ins>Note:</ins> we also included additional data in that folder. That comes from using the Billboard data set and running the Spotify API on it to obtain song information, so that we have more Billboard songs in our data, making for a bigger sample size when creating a balanced data set. That chunk of data also needed preprocessing, which can be seen in the `merge_billboard` folder.)\n",
    "\n",
    "Thus, our goal is to **merge all of these individual files** together into one data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = []\n",
    "for root, dirs, files in os.walk(os.path.abspath(\"./merge\")):\n",
    "    for file in files:\n",
    "        file_paths.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = spark.read.csv(\n",
    "    file_paths[0],\n",
    "    header=True,  \n",
    "    quote='\"',   \n",
    "    escape='\"',   \n",
    "    multiLine=True, \n",
    "    inferSchema=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To merge the files, we had to **recast some variables** accordingly. This is important especially for faulty observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.withColumn(\"9\", combined_df[\"9\"].cast(\"string\"))\n",
    "combined_df = combined_df.withColumn(\"23\", combined_df[\"23\"].cast(\"string\"))\n",
    "combined_df = combined_df.withColumn(\"14\", combined_df[\"14\"].cast(\"integer\"))\n",
    "combined_df = combined_df.drop(\"_c0\", \"_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the remaining file paths and union each one\n",
    "for file_path in file_paths[1:]:\n",
    "    temp_df = spark.read.csv(\n",
    "        file_path,\n",
    "        header=True,  # Assuming the first row is a header\n",
    "        quote='\"',    # Use double quotes to handle fields containing commas\n",
    "        escape='\"',   # Escape double quotes with another double quote\n",
    "        multiLine=True,  # Enable if fields span multiple lines\n",
    "        inferSchema=True  # Infers the schema of the CSV file\n",
    "    )\n",
    "    try:\n",
    "        try: \n",
    "            temp_df = temp_df.drop(\"_c0\", \"_1\")\n",
    "        except:\n",
    "            pass\n",
    "        temp_df = (temp_df.withColumn(\"_2\", temp_df[0].cast(\"string\"))\n",
    "                            .withColumn(\"_3\", temp_df[1].cast(\"string\"))\n",
    "                            .withColumn(\"_4\", temp_df[2].cast(\"integer\"))\n",
    "                            .withColumn(\"_5\", temp_df[3].cast(\"string\"))\n",
    "                            .withColumn(\"_6\", temp_df[4].cast(\"string\"))\n",
    "                            .withColumn(\"_7\", temp_df[5].cast(\"integer\"))\n",
    "                            .withColumn(\"_8\", temp_df[6].cast(\"integer\"))\n",
    "                            .withColumn(\"_9\", temp_df[7].cast(\"string\"))\n",
    "                            .withColumn(\"_10\", temp_df[8].cast(\"double\"))\n",
    "                            .withColumn(\"_11\", temp_df[9].cast(\"double\"))\n",
    "                            .withColumn(\"_12\", temp_df[10].cast(\"integer\"))\n",
    "                            .withColumn(\"_13\", temp_df[11].cast(\"double\"))\n",
    "                            .withColumn(\"_14\", temp_df[12].cast(\"integer\"))\n",
    "                            .withColumn(\"_15\", temp_df[13].cast(\"double\"))\n",
    "                            .withColumn(\"_16\", temp_df[14].cast(\"double\"))\n",
    "                            .withColumn(\"_17\", temp_df[15].cast(\"double\"))\n",
    "                            .withColumn(\"_18\", temp_df[16].cast(\"double\"))\n",
    "                            .withColumn(\"_19\", temp_df[17].cast(\"double\"))\n",
    "                            .withColumn(\"_20\", temp_df[18].cast(\"double\"))\n",
    "                            .withColumn(\"_21\", temp_df[19].cast(\"integer\"))\n",
    "                            .withColumn(\"_22\", temp_df[20].cast(\"string\"))\n",
    "                            .withColumn(\"_23\", temp_df[21].cast(\"string\"))\n",
    "                            .withColumn(\"_24\", temp_df[22].cast(\"integer\"))\n",
    "                            .withColumn(\"_25\", temp_df[23].cast(\"string\")))\n",
    "        temp_df = temp_df.select(\"_2\", \"_3\", \"_4\", \"_5\", \"_6\", \"_7\", \"_8\", \"_9\", \"_10\", \"_11\", \"_12\", \"_13\", \"_14\", \"_15\", \"_16\", \"_17\", \"_18\", \"_19\", \"_20\", \"_21\", \"_22\", \"_23\", \"_24\", \"_25\")\n",
    "        combined_df = combined_df.union(temp_df)\n",
    "    except:\n",
    "        print(file_path, \"didn't work\\n\", temp_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we'll **rename the columns** and **drop duplicates**. We also cleaned up some columns by dropping observations with NAs and making sure our binary variables don't contain any other strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df2 = combined_df.drop(\"4\", \"24\")\n",
    "\n",
    "df = (combined_df2\n",
    "              .withColumnRenamed(\"_2\", \"track_name\")\n",
    "              .withColumnRenamed(\"_3\", \"artist\")\n",
    "              .withColumnRenamed(\"5\", \"track_id\")\n",
    "              .withColumnRenamed(\"6\", \"album_name\")\n",
    "              .withColumnRenamed(\"7\", \"popularity\")\n",
    "              .withColumnRenamed(\"8\", \"duration_ms\")\n",
    "              .withColumnRenamed(\"9\", \"explicit\")\n",
    "              .withColumnRenamed(\"10\", \"danceability\")\n",
    "              .withColumnRenamed(\"11\", \"energy\")\n",
    "              .withColumnRenamed(\"12\", \"key\")\n",
    "              .withColumnRenamed(\"13\", \"loudness\")\n",
    "              .withColumnRenamed(\"14\", \"mode\")\n",
    "              .withColumnRenamed(\"15\", \"speechiness\")\n",
    "              .withColumnRenamed(\"16\", \"acousticness\")\n",
    "              .withColumnRenamed(\"17\", \"instrumentalness\")\n",
    "              .withColumnRenamed(\"18\", \"liveness\")\n",
    "              .withColumnRenamed(\"19\", \"valence\")\n",
    "              .withColumnRenamed(\"20\", \"tempo\")\n",
    "              .withColumnRenamed(\"21\", \"time_signature\")\n",
    "              .withColumnRenamed(\"22\", \"track_genre\")\n",
    "              .withColumnRenamed(\"23\", \"billboard\")\n",
    "              .withColumnRenamed(\"25\", \"lyrics\"))\n",
    "df = df.filter(df.lyrics != \"No lyrics found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup = df.dropDuplicates(['track_name', 'artist'])\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "dedup2 = dedup.filter((dedup.billboard == True) | (dedup.billboard == False))  \n",
    "dedup2 = dedup.filter((dedup.explicit == True) | (dedup.explicit == False))\n",
    "dedup3 = dedup2.filter(dedup2.loudness.isNotNull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we wanted to check whether the Billboard column (which will be our output variable) has correct entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|billboard|count|\n",
      "+---------+-----+\n",
      "|    False|13295|\n",
      "|    false|39544|\n",
      "|     True| 2348|\n",
      "|     true| 6575|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Counting each value in the `billboard` column\n",
    "dedup3.groupBy(\"billboard\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we need to **standardize this column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|billboard_numeric|count|\n",
      "+-----------------+-----+\n",
      "|              0.0|52839|\n",
      "|              1.0| 8923|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Standardizing the \"billboard\" column\n",
    "    dedup4 = dedup3.withColumn(\"billboard\", when(col(\"billboard\").isin(\"True\", \"true\"), \"True\").otherwise(\"False\"))\n",
    "    # Using StringIndexer to turn billboard to a numerical column\n",
    "    indexer = StringIndexer(inputCol = \"billboard\", outputCol = \"billboard_numeric\", handleInvalid = \"skip\")\n",
    "    dedup4 = indexer.fit(dedup4).transform(dedup4)\n",
    "    \n",
    "except AnalysisException as e:\n",
    "    print(\"Error when processing billboard column transformation:\", e)\n",
    "    \n",
    "# Show the number of entries for each value in the new \"billboard_numeric\" column after the conversion\n",
    "dedup4.groupBy(\"billboard_numeric\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|billboard|count|\n",
      "+---------+-----+\n",
      "|      0.0|52839|\n",
      "|      1.0| 8923|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dedup4 = dedup4.drop(\"billboard\")\n",
    "dedup4 = dedup4.withColumnRenamed(\"billboard_numeric\", \"billboard\")\n",
    "dedup4.groupBy(\"billboard\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we just cleaned up the `lyrics` column and created our output into `final.csv`. (Note: we used the Pandas in this part of the code, because the Spark output struggled with the many escape quotes and commas of the lyrics column. The Pandas version is also significantly faster than it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup4 = dedup4.withColumn('lyrics', F.regexp_replace('lyrics', r'\\*', ''))\n",
    "dedup4 = dedup4.withColumn('lyrics', F.regexp_replace('lyrics', 'This Lyrics is NOT for Commercial use', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup4.toPandas().to_csv(\"final.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
