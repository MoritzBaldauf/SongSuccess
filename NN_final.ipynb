{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we define and train a neural network on the Numerical as well as the lyrics' data from our data set.\n",
    "\n",
    "For those who have not worked with Neural Networks before (co-created with ChatGPT): \n",
    "\n",
    "A neural network is a model type inspired by the structure and function of the human brain. It consists of interconnected neurons organized in layers that process and transmit information. They are capable of learning patterns and relationships in data through a process called training, which involves adjusting the connections (or weights) between neurons based on input data and desired outputs. Neural Networks are commonly used due to their ability to model complex, nonlinear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up the spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark/\")\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local[8]\") \\\n",
    "   .appName(\"NNClassifier\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .config(\"spark.sql.random.seed\", \"1234\") \\\n",
    "   .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are loading the data from the final.csv file, which was created by our preprocessing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+------+--------------+-----------+--------------------+---------+\n",
      "|_c0|          track_name|              artist|            track_id|          album_name|popularity|duration_ms|explicit|danceability|energy|key|loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence| tempo|time_signature|track_genre|              lyrics|billboard|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+------+--------------+-----------+--------------------+---------+\n",
      "|  0|\"\"\"Martha: \"\"\"\"M'...|Friedrich von Flo...|1NzZWhNIP9DIX4yy0...|The World's Best ...|        23|     204706|   False|       0.222| 0.195|  5| -13.631|   1|     0.0368|       0.982|        0.000169|  0.0945|   0.11|89.332|             4|      opera|M'appar√¨ tutto am...|      0.0|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+----------+-----------+--------+------------+------+---+--------+----+-----------+------------+----------------+--------+-------+------+--------------+-----------+--------------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.utils import AnalysisException \n",
    "\n",
    "try: \n",
    "    # Because our lyrics has multiple lines of text we need to apply quote, and escape to make sure it is currectly loaded from the csv\n",
    "    file = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"multiline\", \"true\").option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\").load(\"../Final Preprocessing/final.csv\")\n",
    "    file.show(1)\n",
    "except AnalysisException: \n",
    "    print(\"Please check the Filename and Filepath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61762"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the lenght of the loaded file\n",
    "file.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adapting the Numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to work with our numerical columns, we need to cast them into doubles because, when loading them from a CSV, they are datatype strings. Finally, we make sure that we have no missing values in our numerical columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- popularity: double (nullable = true)\n",
      " |-- duration_ms: double (nullable = true)\n",
      " |-- explicit: string (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: double (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: double (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- time_signature: double (nullable = true)\n",
      " |-- track_genre: string (nullable = true)\n",
      " |-- lyrics: string (nullable = true)\n",
      " |-- billboard: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Selecting all the numerical columns we want to use in our model\n",
    "columns_with_numbers = [\"popularity\", \"duration_ms\", \"danceability\", \"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\",\"acousticness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\", \"billboard\"]\n",
    "\n",
    "# Cast all numerical columns to double\n",
    "for column_name in columns_with_numbers:\n",
    "    file = file.withColumn(column_name, col(column_name).cast(\"double\"))\n",
    "    \n",
    "# Remove possible NA form our numerical columns\n",
    "file = file.na.drop(subset=[\"popularity\", \"duration_ms\", \"danceability\", \"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\",\"acousticness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\", \"billboard\"])\n",
    "\n",
    "# Check that our columns have to correct datatype \n",
    "file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|billboard|count|\n",
      "+---------+-----+\n",
      "|      0.0|52839|\n",
      "|      1.0| 8923|\n",
      "+---------+-----+\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- artist: string (nullable = true)\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- popularity: double (nullable = true)\n",
      " |-- duration_ms: double (nullable = true)\n",
      " |-- explicit: string (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: double (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: double (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- time_signature: double (nullable = true)\n",
      " |-- track_genre: string (nullable = true)\n",
      " |-- lyrics: string (nullable = true)\n",
      " |-- billboard: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting a understanding of how our data is distributed \n",
    "file.select('billboard').groupBy('billboard').count().show()\n",
    "file.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Transformation for Numerical column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we transform our numerical columns into a combined feature column called numeric_vector. On this column, we then perform a Scaler to ensure that all the values are comparable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# Create initial Feature Vector\n",
    "assembler = VectorAssembler(inputCols=[\"popularity\", \"duration_ms\", \"danceability\", \"danceability\", \"energy\", \"key\", \"loudness\", \"mode\", \"speechiness\",\"acousticness\", \"acousticness\", \"instrumentalness\", \"liveness\", \"valence\", \"tempo\", \"time_signature\"], outputCol=\"numeric_vector\")\n",
    "file = assembler.transform(file)\n",
    "\n",
    "# Define Scaler\n",
    "scaler = StandardScaler(inputCol=\"numeric_vector\", outputCol=\"scaled_features\")\n",
    "\n",
    "# Perform feature transformation on the data \n",
    "scaler_model = scaler.fit(file)\n",
    "file = scaler_model.transform(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Transformation for Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to do a Feature Transformation on the Lyrics. For this, we first tokenize the text, then apply a Hashing Transformation and finally an IDF. The results are written to the text_features column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "# Tokenizing\n",
    "tokenizer = Tokenizer(inputCol=\"lyrics\", outputCol=\"words\")\n",
    "file = tokenizer.transform(file)\n",
    "\n",
    "# Hashing\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(file)\n",
    "\n",
    "# IDF\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"text_features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "\n",
    "# Writing feature column to data \n",
    "file = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combine features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous steps, we created the feature columns for numerical and lyrical data. However, our neural network can only take one feature input. Because of that, we need to combine the two feature columns (numerical and lyrics) into one combined feature. We do this by using the VectorAssembler function. The resulting feature column will then be used as the input for our neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Define Vector Assembler\n",
    "final_assembler = VectorAssembler(inputCols=[\"scaled_features\", \"text_features\"], outputCol=\"features\")\n",
    "\n",
    "# Apply to data\n",
    "file = final_assembler.transform(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create evenly split dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already described in the NLP model, we agreed to use an evenly split data set for our model training, to avoid the possibility of biases in the data interfering with our performance. In the next step, we create an evenly split dataset for the model training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = file.groupBy(\"billboard\").count().collect()\n",
    "\n",
    "min_count = min(row['count'] for row in label_counts)\n",
    "\n",
    "# Create balanced DataFrame by sampling\n",
    "balanced_df = None\n",
    "\n",
    "for row in label_counts:\n",
    "    label = row['billboard']\n",
    "    count = row['count']\n",
    "    fraction = min_count / count  # Calculate the fraction of the data to sample\n",
    "    \n",
    "    # Sample the data\n",
    "    sampled_df = file.filter(col(\"billboard\") == label).sample(False, fraction, seed=1)\n",
    "    \n",
    "    # Append sampled data to the balanced DataFrame\n",
    "    if balanced_df is None:\n",
    "        balanced_df = sampled_df.limit(min_count)  # Use limit to ensure exact number of instances\n",
    "    else:\n",
    "        balanced_df = balanced_df.union(sampled_df.limit(min_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|billboard|count|\n",
      "+---------+-----+\n",
      "|      0.0| 8825|\n",
      "|      1.0| 8923|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "balanced_df.select('billboard').groupBy('billboard').count().show()\n",
    "\n",
    "file = balanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. Setting up the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in our model creation process is defining the underlying architecture (the number of layers and neurons). For the input layer, we get the feature dimension of the combined feature columns we previously created. \n",
    "\n",
    "When it comes to the hidden layers, we used the Heuristic of using  two hidden layers with a number of neurons, as outlined in this article: https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3. Note that we have also experimented with different hidden layer structures but could not find any significant differences. From our perspective, this makes sense, as our current neuron structure should be able to explain rather complex data inputs. \n",
    "\n",
    "For the output layer, we used two neurons, since we wanted to perform a binary classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Layer: 36\n",
      "Hidden Layer 1: 13.0\n",
      "Hidden Layer 2: 12.0\n",
      "Output Layer: 2\n"
     ]
    }
   ],
   "source": [
    "# Get the input dimension for the Neural Network\n",
    "feature_dimension = file.schema[\"features\"].metadata[\"ml_attr\"][\"num_attrs\"]\n",
    "\n",
    "# Calculate hidden layers, source: https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3\n",
    "\n",
    "hidden_neurons = feature_dimension * (2/3) + 2  # Approximation from source above\n",
    "hidden_neurons_1 = hidden_neurons // 2 # Split between layer 1 and 2\n",
    "hidden_neurons_2 = round(hidden_neurons - hidden_neurons_1 -1 ,0) # -1 to make sure we dont go above input size\n",
    "\n",
    "# Output of size 2 -> Two output classes because we want to classify if the song is in the billboard 100 or not\n",
    "layers = [feature_dimension, hidden_neurons_1, hidden_neurons_2, 2] # This is our final layer structure \n",
    "\n",
    "print(f\"Input Layer: {feature_dimension}\")\n",
    "print(f\"Hidden Layer 1: {hidden_neurons_1}\")\n",
    "print(f\"Hidden Layer 2: {hidden_neurons_2}\")\n",
    "print(f\"Output Layer: 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Split Train and Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model training and model evaluation process, we create a training and test data set with the ratio 80/20. We also set a seed for this split to make the model performance comparable with the Logistic Regression and the NLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = file.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3. Grid search method to find the best hyperparameters for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to achieve the best possible model. For this, we need to find the parameters that match the data the best. Since we don't want to manually change the model parameters and re-run the model, we create a grid with the parameters we think have the biggest impact. What is extremely important here is that you do not try to test too many parameters, as the Time Complexity scales with O(x^2). Because of that we only focused on the blockSize, stepSize, and tol (tolerance). \n",
    "\n",
    "The model performance was evaluated using a 2-fold Cross Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(featuresCol='features', labelCol='billboard')\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(mlp.maxIter, [100]) \\\n",
    "    .addGrid(mlp.layers, [layers]) \\\n",
    "    .addGrid(mlp.blockSize, [2,3,4,6]) \\\n",
    "    .addGrid(mlp.stepSize, [0.00001,0.00005, 0.000001, 0.0000001]) \\\n",
    "    .addGrid(mlp.tol, [1e-06, 1e-08, 1e-10]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = 'billboard', metricName=\"accuracy\")\n",
    "\n",
    "crossval = CrossValidator(estimator=mlp,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2)  # Use 2 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the gird search and train the model\n",
    "cvModel = crossval.fit(train_data)\n",
    "\n",
    "# Fetch best model\n",
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our models' performance we used the measures, Area Under ROC and Area under PR. For the Neural Network, we calculate these measurements on the test data using the best resulting model from our Cross Validation Grid search.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set AUC (ROC) = 0.8259662715916801\n",
      "Test set AUC (PR) = 0.8065419144102637\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Transform the test data using the best model\n",
    "predictions = bestModel.transform(test_data)\n",
    "\n",
    "# Initialize the evaluator for accuracy and F1 score\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='billboard')\n",
    "\n",
    "# Evaluate the model\n",
    "areaUnderROC = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "areaUnderPR = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "print(\"Test set AUC (ROC) = \" + str(areaUnderROC))\n",
    "print(\"Test set AUC (PR) = \" + str(areaUnderPR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further investigate and evaluate our model's performance, and to make it easier to benchmark the Neural Network results with the other models(NLPs) and (LR), we also created a confusion matrix based on the calculated predictions. **Note:** All of this is run based on training and testing on the balanced data. Our final comparison and results will also be based on this balanced data across all of our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|billboard|prediction|count|\n",
      "+---------+----------+-----+\n",
      "|      1.0|       1.0| 1472|\n",
      "|      0.0|       1.0|  552|\n",
      "|      1.0|       0.0|  326|\n",
      "|      0.0|       0.0| 1188|\n",
      "+---------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "confusion_matrix = predictions.groupBy(\"billboard\", \"prediction\").count()\n",
    "confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of the confusion matrix, then, we can now calculate the accuracy and F1 scores of this NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7518371961560204\n",
      "Precision: 0.7272727272727273\n",
      "Recall: 0.8186874304783093\n",
      "F1 Score: 0.7702773417059132\n"
     ]
    }
   ],
   "source": [
    "# Assigning the needed values from the confusion matrix - https://towardsdatascience.com/a-look-at-precision-recall-and-f1-score-36b5fd0dd3ec\n",
    "\n",
    "# True Positive\n",
    "TP = confusion_matrix.filter((col(\"billboard\") == 1) & (col(\"prediction\") == 1)).collect()[0][\"count\"]\n",
    "\n",
    "# False Positive\n",
    "FP = confusion_matrix.filter((col(\"billboard\") == 0) & (col(\"prediction\") == 1)).collect()[0][\"count\"]\n",
    "\n",
    "# True Negative\n",
    "TN = confusion_matrix.filter((col(\"billboard\") == 0) & (col(\"prediction\") == 0)).collect()[0][\"count\"]\n",
    "\n",
    "# False Negative\n",
    "FN = confusion_matrix.filter((col(\"billboard\") == 1) & (col(\"prediction\") == 0)).collect()[0][\"count\"]\n",
    "\n",
    "\n",
    "# Calculating the metrics\n",
    "Accuracy = (TP + TN)/(TP + TN + FP + FN) \n",
    "Precision = TP/(TP + FP) \n",
    "Recall = TP/(TP + FN) \n",
    "F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {Accuracy}\")\n",
    "print(f\"Precision: {Precision}\")\n",
    "print(f\"Recall: {Recall}\")\n",
    "print(f\"F1 Score: {F1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Describe results\n",
    "\n",
    "Compared to the other models - both Logistic Regression and NLP, this Neural Network model gives us the best numbers in almost everything, with higher **ROC-AUC**, **PR-AUC**, and better **Accuracy** and **F1** scores(In fact, F1 score is comparable with NLP). This shows the superior prediction capabilities of NN models, which may be due to the fact that both Logistic Regression and the NLP models use either numerical or lyrical data, while the Neural Network uses both of them when making predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid having to rerun the time-intensive training process each time we start the server, we save the fully trained model for re-use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "try:\n",
    "    bestModel.save(\"../Final Code/trained_models/NN_model\")\n",
    "except Py4JJavaError:\n",
    "    print(\"Error saving model the model, make sure the model isn't allready saved in the defined folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
